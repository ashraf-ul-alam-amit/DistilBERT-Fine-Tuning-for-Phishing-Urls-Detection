{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f65e0d82aecc4aab8537f31582bdd3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f180267cb8ab49f1871091e9f08020ef",
              "IPY_MODEL_36699c0b4e194b8f87cbad0ed2a6a654",
              "IPY_MODEL_63dec6d2bfd54a81a9e971940bc89ae7"
            ],
            "layout": "IPY_MODEL_ecca3539a0f74acfa183d91c348b0f7c"
          }
        },
        "f180267cb8ab49f1871091e9f08020ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1b28bd52e6644d283f12728a456d208",
            "placeholder": "​",
            "style": "IPY_MODEL_574a615bb0e54c41a63a40ff240fefdd",
            "value": "Downloading builder script: 100%"
          }
        },
        "36699c0b4e194b8f87cbad0ed2a6a654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f621605865449daa41167d1684f0d5b",
            "max": 6785,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57adb6b6557348279155b0c8a9f641c4",
            "value": 6785
          }
        },
        "63dec6d2bfd54a81a9e971940bc89ae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc1f29e9c43047a19822d30fabab6b47",
            "placeholder": "​",
            "style": "IPY_MODEL_9cb02e300e8542c89137ce726f7bcf51",
            "value": " 6.79k/6.79k [00:00&lt;00:00, 433kB/s]"
          }
        },
        "ecca3539a0f74acfa183d91c348b0f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1b28bd52e6644d283f12728a456d208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "574a615bb0e54c41a63a40ff240fefdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f621605865449daa41167d1684f0d5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57adb6b6557348279155b0c8a9f641c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc1f29e9c43047a19822d30fabab6b47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb02e300e8542c89137ce726f7bcf51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phishing Urls Detection by Fine Tuning DistilBERT\n",
        "---"
      ],
      "metadata": {
        "id": "-knib2KqzcEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display resources\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "i0O-zDKFn0ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367e732a-1e6c-4be5-dea7-232fb3f88f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec  1 20:11:33 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "BNc2M2PiT4_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FadF0CA7ivau"
      },
      "outputs": [],
      "source": [
        "%pip install transformers datasets evaluate accelerate pipeline bitsandbytes\n",
        "%pip install torch torchdata\n",
        "%pip install peft\n",
        "%pip install loralib\n",
        "%pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    GenerationConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "import torch\n",
        "import evaluate\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        ")\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "edwnrK-Vj0kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login()"
      ],
      "metadata": {
        "id": "UNYkdQnumI57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training directory\n",
        "DIR_MODEL = f\"/content/drive/MyDrive/Colab Notebooks/fine-tuning-llm/malware_detection/peft/models/\"\n",
        "\n",
        "# device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "00VSWiobj3BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine Tuning Dataset: kmack/Phishing_urls\n",
        "+ [Phishing_urls](https://huggingface.co/datasets/kmack/Phishing_urls) => available from HuggingFace"
      ],
      "metadata": {
        "id": "Y6xFI3gdLKqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_urls = load_dataset(\"kmack/Phishing_urls\")\n",
        "\n",
        "# Load training data\n",
        "data_train = data_urls['train']\n",
        "\n",
        "# Split test data into test and validate sets\n",
        "data_test = data_urls['test'].train_test_split(test_size=0.3, seed=1985)\n",
        "\n",
        "train = data_train.shuffle(seed=1985).select(range(5000))\n",
        "test = data_test['train'].shuffle(seed=1985).select(range(1000))\n",
        "validate = data_test['test'].shuffle(seed=1985).select(range(100))\n"
      ],
      "metadata": {
        "id": "ONaTTiaOxg8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base Model\n",
        "+ The distilBERT base model (case insensitive version) was fine-tuned with the Phishing_urls to improve classification\n",
        "\n"
      ],
      "metadata": {
        "id": "C-O9fuwrGa5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBERT Base Model\n",
        "base_model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "# classification mappings\n",
        "id2label = {0:\"Negative\",1:\"Positive\"}\n",
        "label2id = {\"Negative\":0, \"Positive\":1}\n",
        "\n",
        "# base model for training\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model_name,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    torch_dtype=torch.bfloat16\n",
        "    ).to(DEVICE)\n",
        "\n",
        "# original model for evaluation\n",
        "original_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model_name,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    torch_dtype=torch.bfloat16\n",
        "    ).to(DEVICE)\n"
      ],
      "metadata": {
        "id": "n4vDRZv27t0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4520147a-4b06-4ba6-c86a-1e6e6b8b688f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "+ Preprocessing is required to tokenize the inputs and standardize the length of each review.\n",
        "+ Steps:\n",
        "  + Tokenize each review\n",
        "  + Standardize review length: A combination of truncation and padding was used to ensure the length of text for each review was the same length.\n",
        "  + The DataCollatorWithPadding function from HuggingFace was used to automatically set padding levels during training."
      ],
      "metadata": {
        "id": "nCa7nybxGgAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example raw url string\n",
        "tokens = tokenizer('thecanadianencyclopedia.com/index.cfm?PgNm=TCE&Params=A1ARTA0006086', truncation=True)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQegg3nf9rFg",
        "outputId": "8850f1d0-edc2-4a15-c8cf-91a70eb703c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 1996, 28621, 11692, 11916, 20464, 24174, 2401, 1012, 4012, 1013, 5950, 1012, 12935, 2213, 1029, 18720, 2078, 2213, 1027, 22975, 2063, 1004, 11498, 5244, 1027, 17350, 8445, 2050, 8889, 2692, 16086, 20842, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(examples):\n",
        "  \"\"\" Tokenize the input text \"\"\"\n",
        "  tokens = tokenizer(examples['text'], truncation=True)\n",
        "  return tokens\n",
        "\n",
        "# preprocess each review in the train, test and validate datasets\n",
        "tokenized_train = train.map(preprocess, batched=True)\n",
        "tokenized_test = test.map(preprocess, batched=True)\n",
        "tokenized_val = validate.map(preprocess, batched=True)"
      ],
      "metadata": {
        "id": "9i6E4jcOe2dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Responses\n",
        "+ Steps\n",
        "\t+ Tokenize the review\n",
        "\t+ Generate a response\n",
        "\t+ Extract the logits\n",
        "\t+ Infer the classification from the maximum logit value\n",
        "\t+ If verbose=1, then print the review, the decoded classification, and human labels"
      ],
      "metadata": {
        "id": "Q2JBgKBlk9p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(example, model, tokenizer, verbose=False):\n",
        "  \"\"\" Generate a classification for a sample review \"\"\"\n",
        "  # tokenize the input text\n",
        "  encoded_input = tokenizer(example['text'], return_tensors=\"pt\", truncation=True, padding =True)\n",
        "  encoded_input.to(DEVICE)\n",
        "\n",
        "  # get the logits\n",
        "  logits = model(**encoded_input).logits\n",
        "\n",
        "  # classify\n",
        "  prediction = torch.argmax(logits).tolist()\n",
        "\n",
        "  # print a summary\n",
        "  if verbose:\n",
        "    # decode the prediction\n",
        "    decoded_output = id2label[prediction]\n",
        "    print(\"Input Text\")\n",
        "    print(\"=\"*100)\n",
        "    print(example['text'])\n",
        "    print(\"=\"*100)\n",
        "    print(f\"Prediction: {decoded_output} | Label: {id2label[example['label']]}\")\n",
        "  else:\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "nCyWjVxGWiDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_training_metrics(pred):\n",
        "  \"\"\" Calculate the evaluation metrics during training \"\"\"\n",
        "  f1 = evaluate.load('f1')\n",
        "\n",
        "  # get the logits and labels from the prediction object\n",
        "  logits, labels = pred\n",
        "\n",
        "  # classify by using the logit (assign using the largest value)\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "  score = f1.compute(predictions=predictions, references=labels)['f1']\n",
        "  return {'f1':score}"
      ],
      "metadata": {
        "id": "4XPBp1oFuSjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training with Parameter Efficient Fine Tuning (PEFT) -> LoRA\n",
        "+ Steps\n",
        "\t+ Define the LoRA parameters in the LoraConfig object\n",
        "\t+ Prepare the PEFT model from the base model + LoRA config object\n",
        "\t+ View the number of trainable parameters in the PEFT model\n"
      ],
      "metadata": {
        "id": "8WVpC8gDmj_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r = 8, # dimension of adaptors, rank\n",
        "    target_modules = [\"q_lin\"],\n",
        "    lora_alpha=16, # alpha scaling\n",
        "    lora_dropout=0.05,\n",
        "    task_type=TaskType.SEQ_CLS # text classification\n",
        ")\n",
        "\n",
        "# Create the PEFT model from the base model and LoRA config\n",
        "peft_model = get_peft_model(base_model, lora_config)\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FKwHj7LPx2T",
        "outputId": "0f56aeb5-568f-475a-a045-b998aaf6a543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 665,858 || all params: 67,620,868 || trainable%: 0.9847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training\n",
        "This project aimed to demonstrate how to fine-tune LLMs for specific tasks using public datasets. As the focus was not on performance, no attempt at hyperparameter tuning was undertaken. In most instances, the default hyperparameter values were used\n",
        "+ **Key Parameters**\n",
        "+ output_dir - location to save trained adaptor weights\n",
        "+ learning_rate -set to default\n",
        "+ auto_find_batch_size - set to auto\n",
        "+ Logging and evaluation were set to occur after each epoch\n",
        "+ load_best_model_at_end - set to true to capture the best model from the epoch training\n",
        "+ The data collator is used to automatically pad the text to the longest sequence in each batch"
      ],
      "metadata": {
        "id": "I8kM3lmNwT6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collator: This function dynamically sets the padding during training and ensures prompts of are equal length\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# training config\n",
        "DIR_TRAIN = \"./training_output\"\n",
        "\n",
        "config_training = TrainingArguments(\n",
        "    output_dir=DIR_TRAIN,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=config_training,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    compute_metrics=calc_training_metrics\n",
        ")\n",
        "\n",
        "# train\n",
        "trainer.train()\n",
        "\n",
        "# save adaptor weights\n",
        "trainer.save_model(DIR_MODEL)\n",
        "# peft_model.push_to_hub(hub_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736,
          "referenced_widgets": [
            "f65e0d82aecc4aab8537f31582bdd3b4",
            "f180267cb8ab49f1871091e9f08020ef",
            "36699c0b4e194b8f87cbad0ed2a6a654",
            "63dec6d2bfd54a81a9e971940bc89ae7",
            "ecca3539a0f74acfa183d91c348b0f7c",
            "f1b28bd52e6644d283f12728a456d208",
            "574a615bb0e54c41a63a40ff240fefdd",
            "0f621605865449daa41167d1684f0d5b",
            "57adb6b6557348279155b0c8a9f641c4",
            "cc1f29e9c43047a19822d30fabab6b47",
            "9cb02e300e8542c89137ce726f7bcf51"
          ]
        },
        "id": "Gk9o-Zp7QFzc",
        "outputId": "0640e0d3-7463-40be-f30d-e666fdee79e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mashrafamit9227\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250201_101616-k9owdzw1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ashrafamit9227/huggingface/runs/k9owdzw1' target=\"_blank\">./training_output</a></strong> to <a href='https://wandb.ai/ashrafamit9227/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ashrafamit9227/huggingface' target=\"_blank\">https://wandb.ai/ashrafamit9227/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ashrafamit9227/huggingface/runs/k9owdzw1' target=\"_blank\">https://wandb.ai/ashrafamit9227/huggingface/runs/k9owdzw1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6250/6250 05:31, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.718800</td>\n",
              "      <td>0.441094</td>\n",
              "      <td>0.807692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.308600</td>\n",
              "      <td>0.517344</td>\n",
              "      <td>0.808081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.484400</td>\n",
              "      <td>0.530117</td>\n",
              "      <td>0.804124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.490200</td>\n",
              "      <td>0.516699</td>\n",
              "      <td>0.823529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.032700</td>\n",
              "      <td>0.517559</td>\n",
              "      <td>0.811881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.033400</td>\n",
              "      <td>0.644609</td>\n",
              "      <td>0.783505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>0.646895</td>\n",
              "      <td>0.791667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.041700</td>\n",
              "      <td>0.725210</td>\n",
              "      <td>0.795918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>0.745674</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.119100</td>\n",
              "      <td>0.760913</td>\n",
              "      <td>0.795918</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f65e0d82aecc4aab8537f31582bdd3b4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge Base Model & Adapters\n",
        "+ The trained LoRA adaptors must be merged with the original base model\n",
        "+ The resulting model consists of the base model plus the trained adaptors\n"
      ],
      "metadata": {
        "id": "RF02lWhTGtxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merge base model + peft adaptors\n",
        "tuned_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    DIR_MODEL, # LoRA adapters\n",
        "    torch_dthype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    is_trainable=False\n",
        "  )"
      ],
      "metadata": {
        "id": "_R1GTHtaYQnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model Performance\n",
        "+ This is a supervised binary classification task (we have the ground truth labels). Therefore, a classification accuracy measure can be used. The F1 score was selected to balance precision and recall\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kobJ7rUW0dHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(test_indexes, data, model, tokenizer):\n",
        "  \"\"\" Generate classifications for each example in the test indexes \"\"\"\n",
        "  # accumulator\n",
        "  results = []\n",
        "\n",
        "  # loop through each test index in the dataset\n",
        "  for idx in test_indexes:\n",
        "    # get the human label and the generated classification\n",
        "    example = data[idx]\n",
        "    label = example['label']\n",
        "    pred = get_response(example, model, tokenizer, verbose=False)\n",
        "\n",
        "    # accumuate results\n",
        "    results.append({'idx':idx,'label':label,'pred':pred})\n",
        "  return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "PdiuS9CHKJir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from evaluate import load\n",
        "\n",
        "f1 = load(\"f1\")\n",
        "\n",
        "# Select samples from the test dataset\n",
        "num_samples = test.num_rows - 1\n",
        "num_to_test = min(500, num_samples)\n",
        "test_indexes = random.sample(range(num_samples), num_to_test)\n",
        "\n",
        "# Evaluate the Base Model\n",
        "df_base = evaluate_model(test_indexes, test, original_model, tokenizer)\n",
        "f1_base = f1.compute(predictions=df_base['pred'], references=df_base['label'])['f1']\n",
        "print(f\"Base Model F1 Score: {f1_base*100:,.2f}%\")\n",
        "\n",
        "# Evaluate the Tuned Model\n",
        "df_tuned = evaluate_model(test_indexes, test, tuned_model, tokenizer)\n",
        "f1_tuned = f1.compute(predictions=df_tuned['pred'], references=df_tuned['label'])['f1']\n",
        "print(f\"Tuned Model F1 Score: {f1_tuned*100:,.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL5rkNHDrzrP",
        "outputId": "47e58036-2a80-4836-9e6d-05c454f55b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Model F1 Score: 57.88%\n",
            "Tuned Model F1 Score: 84.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PEFT/LoRA fine-tuning increased the F1 score from 57% to 84%\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "ihnCpg0xC38Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "Zpiv09vEEFVm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
